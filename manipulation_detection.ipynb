# %% [markdown]
# # Market Manipulation Detection Research Notebook
# Advanced detection of spoofing/layering patterns using Deep Learning and Limit Order Book analysis

# %%
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Input, LSTM, Dense, Conv1D, MaxPooling1D, Flatten, concatenate
from sklearn.ensemble import IsolationForest
from sklearn.metrics import classification_report, roc_auc_score
import shap
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from tqdm import tqdm
from scipy.stats import levy_stable
from umap import UMAP
from aif360.metrics import ClassificationMetric
from scipy.signal import savgol_filter

# %%
class ManipulationDataGenerator:
    def __init__(self, n_normal=10000, n_spoof=500, n_layering=500):
        self.n_normal = n_normal
        self.n_spoof = n_spoof
        self.n_layering = n_layering
        
    def _generate_normal_flow(self):
        return levy_stable.rvs(alpha=1.7, beta=0, scale=0.01, size=self.n_normal)
    
    def _create_spoofing_pattern(self):
        base = self._generate_normal_flow()
        spoofs = np.random.choice(len(base), size=50, replace=False)
        base[spoofs] += levy_stable.rvs(alpha=1.3, beta=1, scale=0.1, size=50)
        cancellations = [i+1 for i in spoofs]
        base[cancellations] -= 0.8 * base[spoofs]
        return base
    
    def _create_layering_pattern(self):
        base = self._generate_normal_flow()
        for _ in range(5):
            idx = np.random.randint(100, len(base)-100)
            base[idx:idx+100] += np.sin(np.linspace(0, 3*np.pi, 100)) * 0.2
        return base
    
    def generate_dataset(self):
        normal = self._generate_normal_flow()
        spoof = self._create_spoofing_pattern()
        layer = self._create_layering_pattern()
        X = np.concatenate([normal, spoof, layer])
        y = np.array([0]*len(normal) + [1]*len(spoof) + [1]*len(layer))
        return pd.DataFrame(X, columns=['microstructure']), y

# %%
def build_hybrid_model(input_shape):
    inputs = Input(shape=input_shape)    
    conv = Conv1D(64, 5, activation='relu', padding='same')(inputs)
    conv = MaxPooling1D(2)(conv)
    conv = Conv1D(128, 5, activation='relu', padding='same')(conv)    
    lstm = LSTM(64, return_sequences=True)(inputs)
    lstm = tf.keras.layers.Attention()([lstm, lstm])    
    merged = concatenate([conv, lstm])
    dense = Dense(64, activation='relu')(merged)
    output = Dense(1, activation='sigmoid')(dense)    
    model = Model(inputs=inputs, outputs=output)
    model.compile(loss='binary_crossentropy', 
                 optimizer=tf.keras.optimizers.Adam(0.001),
                 metrics=[tf.keras.metrics.AUC(name='auc')])
    return model

# %%
def hurst_exponent(series):
    lags = range(2, 100)
    tau = [np.std(np.subtract(series[lag:], series[:-lag])) for lag in lags]
    return np.polyfit(np.log(lags), np.log(tau), 1)[0]

def calculate_order_imbalance(df):
    return df['microstructure'].rolling(100).mean()

def calculate_volume_entropy(df):
    return df['microstructure'].rolling(100).std()

# %%
class RealTimeDetector:
    def __init__(self, model, threshold=0.85):
        self.model = model
        self.threshold = threshold
        
    def process_stream(self, data_stream):
        predictions = []
        for batch in tqdm(data_stream):
            features = extract_features(batch)
            preds = self.model.predict(features)
            predictions.extend(preds > self.threshold)
        return np.array(predictions)

# %%
def explain_model(model, background, instances):
    explainer = shap.DeepExplainer(model, background)
    shap_values = explainer.shap_values(instances)
    plt.figure()
    shap.summary_plot(shap_values, instances, plot_type='violin')
    plt.savefig('explanations.png')

# %%
def analyze_manipulation_clusters(predictions, features):
    reducer = UMAP(n_components=2)
    embedding = reducer.fit_transform(features)
    fig = go.Figure()
    fig.add_trace(go.Scattergl(x=embedding[:,0], y=embedding[:,1], mode='markers',
                             marker=dict(color=predictions, colorscale='Viridis')))
    fig.show()

# %%
def assess_model_fairness(model, test_data):
    metric = ClassificationMetric(test_data, model.predict(test_data),
                             privileged_groups=[{'market_cap': 1}],
                             unprivileged_groups=[{'market_cap': 0}])
    print(f"Disparate Impact: {metric.disparate_impact_ratio():.2f}")

# %%
def simulate_market_defense(attack_sequence):
    cleaned = savgol_filter(attack_sequence, 51, 3)
    plt.figure(figsize=(12,6))
    plt.plot(attack_sequence, label='Original')
    plt.plot(cleaned, label='Sanitized')
    plt.legend()
    plt.show()

# %%
if __name__ == "__main__":
    # Generate data
    gen = ManipulationDataGenerator()
    X, y = gen.generate_dataset()
    
    # Build model
    model = build_hybrid_model((100, 1))
    model.fit(X.values.reshape(-1, 100, 1), y, epochs=10, batch_size=32)
    
    # Explainability
    explain_model(model, X[:100].values.reshape(-1, 100, 1), X[100:200].values.reshape(-1, 100, 1))
    
    # Fairness assessment
    assess_model_fairness(model, X)
    
    # Defense simulation
    simulate_market_defense(X.values[:1000])
